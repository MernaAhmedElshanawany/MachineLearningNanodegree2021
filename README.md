# MachineLearningNanodegree2021

## Project1: Finding Donors for CharityML:

* Introduction to the dataset: A dataset that is concerned with the customer segments and some of their features such as how much they earn per month. 

* The Goal: The goal is to identify which segments have more probability to be future donors for CharityML in order to target those segments in the future. 

* Tools used: 
    
    Python -> To display the data set and work on it. 

    Sklearn -> To apply different models on our dataset (SGDC, SVM, Adaboost) and see their performance and decide to tune our final model (Adaboost).
    
    Jupyter Notebook -> To display all the work done using python on the data set and display the visuals in order to find the needed conclusions.
    
## Project2: Image classifier:

* Introduction to the dataset: 102 flower categories commonly occurring in the United Kingdom. Each class consists of between 40 and 258 images. 

* The Goal: The goal is to build an image classifier using deep learning models to identify the different classes of flowers in this dataset and then build a CLI file to predict photos of flowers. 

* Tools used: 
    
    Python -> To display the data set and work on it and finally build a CLI that can predict flower images. 
    
    Tensorflow and TFDS -> To load the dataset and apply our deepl learning techniques on it.

    Jupyter Notebook -> To display all the work done using python on the data set and display the visuals in order to find the needed conclusions.
    
## Project3: Identify customer segments with Arvato:

* Introduction to the dataset: 3 files, one of them contains the demographic data of the general population in Germany, another one for the demographic data of the company's customers and the final file contains the details of the features information. 

* The Goal: The goal is to compare the people segments in the general population to the customer segments in the company to make data based decisions in the future. 

* Tools used: 
    
    Python -> To display the data set and work on it.
    
    Sklearn -> To apply clustering models on our general population and customer datasets and see their performance.

    Jupyter Notebook -> To display all the work done using python on the data set and display the visuals in order to find the needed conclusions
            
            
